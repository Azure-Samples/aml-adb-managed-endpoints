{"cells":[{"cell_type":"markdown","source":["# Deploy a model from Azure Databricks to [Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?tabs=python). \nIn this notebook we will see how to deploy a trained model on Azure Databricks to Managed Online Endpoints securely created on Azure Machine Learning.\n\nWith this approach, we can use the best of Databricks and Azure Machine Learning, training robust models and taking full advantage of this integration's scalability and security."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc0c6977-cf22-4b94-a94e-e4eac6b6482e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In this deployment model, we will use the [Python SDK v2](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python) to integrate with Azure ML in a simple and efficient way. To ensure security, we store security keys and sensitive information such as ``subscription_id``\n, ``resource_group``, and ``AML workspace name`` in an [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/basic-concepts)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e09c1c21-b00c-445d-b83f-5dd44b891db4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\nsubscription_id = dbutils.secrets.get(scope = 'akd-adb-prv', key = 'SUBSCRIPTION-ID')\nresource_group = dbutils.secrets.get(scope = 'akd-adb-prv', key = 'RESOURCE-GROUP')\nworkspace = dbutils.secrets.get(scope = 'akd-adb-prv', key = 'AML-WORKSPACE-NAME')\n\nml_client = MLClient(\n    DefaultAzureCredential(), subscription_id, resource_group, workspace\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c09b4de0-470c-4c1d-97cd-d3d6055a71b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we need to define the Managed Online Endpoint endpoint's configs. \n\nWe set the ``publick_network_access`` as ``disabled`` to ensure that the inbound (scoring) access will use the private endpoint of the [Azure ML Workspace](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-private-link)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"645db5a2-b47c-4f47-9632-b13b72ac49aa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from azure.ai.ml.entities import (ManagedOnlineEndpoint, \n                                 ManagedOnlineDeployment, \n                                 Model, \n                                 CodeConfiguration, \n                                 Environment)\n\nendpoint_name = 'churn-endpoint-01' # YOUR ENDPOINT NAME\n\nendpoint = ManagedOnlineEndpoint(name=endpoint_name,  \n                         description='Realtime endpoint to predict churn', \n                         tags={'model': 'XGBoost'}, \n                         auth_mode=\"key\", \n                         public_network_access=\"disabled\" \n)\n\nml_client.online_endpoints.begin_create_or_update(endpoint)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"161bd632-93c0-467b-8f90-fb9045de5b29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We need to wait for the ``provisioning_state`` to be **Succeeded**. You can define the ``sleep_seconds`` and ``max_loop`` variables to wait for more cycles/seconds according to the time you need to wait."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25f709b8-706f-4674-aa7e-ba1ac24fb649","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import time\n\nmax_loop = 50\nnum_loop = 0\nsleep_seconds = 10\n\nprovisioning_state = ml_client.online_endpoints.get(endpoint_name).provisioning_state\n\nwhile provisioning_state != 'Succeeded' and num_loop < max_loop:\n    print(f'Checking the endpoint creation. Interaction {num_loop}')\n    time.sleep(sleep_seconds)\n    provisioning_state = ml_client.online_endpoints.get(endpoint_name).provisioning_state\n    num_loop+=1\n\nprint(f'The endpoint {endpoint_name} was created with {provisioning_state} provision_state')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12ddf5c2-67e4-44a9-988c-2634fb79cf12","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we will copy the Databricks model to a temp folder on DBFS. In our case, we persisted the model to Azure Data Lake Storage before, so we are copying from the ADLS to DBFS. You can also copy directly from mlflow to DBFS. However, it's important to have this staging folder to access from AML SDK.\n\nWe need to copy the ``conda.yml`` file and have a ``score.py`` (entry script) in the staging folder as well. These files are used to define the dependencies (libraries) that the endpoint needs to consider (conda.yml) and also the entry script that will receive the data submitted to the endpoint and will pass it to the model (score.py). Here you can customize it according to your process as well. For more information about that, please look at this doc about [advanced entry script authoring](https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-deploy-advanced-entry-script)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"558a3a3c-c181-4983-8b75-3461319a26ed","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.set(\n    f\"fs.azure.account.key.{dbutils.secrets.get(scope = 'akd-adb-prv', key = 'STORAGE-STAGING-NAME')}.dfs.core.windows.net\",\n    dbutils.secrets.get(scope = 'akd-adb-prv', key = 'STORAGE-STAGING-KEY'))\n\ndbutils.fs.cp(f'abfs://model-data@{dbutils.secrets.get(scope = 'akd-adb-prv', key = 'STORAGE-STAGING-NAME')}.dfs.core.windows.net/AML-Debug-ME/churn-deploy/environment/conda.yml', 'dbfs:/tmp/model/conda.yml')\ndbutils.fs.cp(f'abfs://model-data@{dbutils.secrets.get(scope = 'akd-adb-prv', key = 'STORAGE-STAGING-NAME')}.dfs.core.windows.net/AML-Debug-ME/churn-deploy/onlinescoring/score.py', 'dbfs:/tmp/model/onlinescoring/score.py')\ndbutils.fs.cp(f'abfs://model-data@{dbutils.secrets.get(scope = 'akd-adb-prv', key = 'STORAGE-STAGING-NAME')}.dfs.core.windows.net/AML-Debug-ME/churn-deploy/model', 'dbfs:/tmp/model/model', True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc337b14-d74a-4541-ad76-9258f8a2aea0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We create a Model on Azure Machine Learning with all models' artifacts"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c20b6d2e-9dc4-4caa-8e24-d2c15f1d2d32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from azure.ai.ml.entities import Model\nfrom azure.ai.ml.constants import AssetTypes\n\ncloud_model = Model(\n    path='/dbfs/tmp/model/model',\n    name=\"churn-model-adb\",\n    type=AssetTypes.CUSTOM_MODEL,\n    description=\"Model created from cloud path.\",\n)\n\nmodel = ml_client.models.create_or_update(cloud_model)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2d76924-f536-4d75-bb94-64ddc8bc4727","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And the last stage is to deploy the model according to the required configs.\n\nHere we need to define the ``instance_type`` and how many nodes we would like to use (``instance_count``).\n\nWe set the ``egress_public_network_access`` to ``disabled`` as well to ensure that the communication between a deployment and external resources, including Azure resources, will use the private endpoint."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"722defbf-322c-4d4f-be67-c0017b97c3fd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["deployment_name = 'blue'\n\nenv = Environment(\n    conda_file=\"/dbfs/tmp/model/conda.yml\",\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n)\n\ndeployment = ManagedOnlineDeployment(name=deployment_name, \n                                          endpoint_name=endpoint_name, \n                                          model=model, \n                                          code_configuration=CodeConfiguration(code='/dbfs/tmp/onlinescoring/',\n                                                                               scoring_script='score.py'),\n                                          environment=env, \n                                          instance_type='Standard_DS2_v2', \n                                          instance_count=1, \n                                          egress_public_network_access=\"disabled\"\n)\n\nml_client.online_deployments.begin_create_or_update(deployment=deployment)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5964455f-43ef-4a40-87d5-3890df19e2ac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Wait for the deployment succeed"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efdf426c-63ea-44cc-81f6-15458db7113c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["max_loop = 100\nsleep_seconds = 30\nnum_loop = 0\n\nprovisioning_state = ml_client.online_deployments.get(name = deployment_name, endpoint_name=endpoint_name).provisioning_state\n\nwhile provisioning_state != 'Succeeded' and num_loop < max_loop:\n    print(f'Checking the deployment. Interaction {num_loop}')\n    time.sleep(sleep_seconds)\n    provisioning_state = ml_client.online_deployments.get(name = deployment_name, endpoint_name=endpoint_name).provisioning_state\n    num_loop+=1\n\nprint(f'The deployment for endpoint {endpoint_name} has finished with {provisioning_state} provision_state')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbcfbe71-8556-4ad4-a017-2297a5ea2a84","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finally, with the endpoint, we can make an API request to ensure we can get the result. Notice that this request will be done in a private context as well."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fb94477-fec6-420e-a35a-a43c6e74e96c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import urllib.request\nimport json\nimport os\nimport ssl\n\ndef allowSelfSignedHttps(allowed):\n    # bypass the server certificate verification on client side\n    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n        ssl._create_default_https_context = ssl._create_unverified_context\n\nallowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n\n# Request data goes here\ndata = {\"data\": [{\"Idade\": 21,\n                                   \"RendaMensal\": 9703, \n                                   \"PercentualUtilizacaoLimite\": 1.0, \n                                   \"QtdTransacoesNegadas\": 5.0, \n                                   \"AnosDeRelacionamentoBanco\": 12.0, \n                                   \"JaUsouChequeEspecial\": 0.0, \n                                   \"QtdEmprestimos\": 1.0, \n                                   \"NumeroAtendimentos\": 100, \n                                   \"TMA\": 300, \n                                   \"IndiceSatisfacao\": 2, \n                                   \"Saldo\": 6438, \n                                   \"CLTV\": 71}\n]}\n\nbody = str.encode(json.dumps(data))\n\nurl = 'https://churn-endpoint-01.southcentralus.inference.ml.azure.com/score'\n\napi_key = dbutils.secrets.get(scope = 'akd-adb-prv', key = 'AML-ENDPOINT-KEY')\n\nif not api_key:\n    raise Exception(\"A key should be provided to invoke the endpoint\")\n\n# The azureml-model-deployment header will force the request to go to a specific deployment.\n# Remove this header to have the request observe the endpoint traffic rules\nheaders = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n\nreq = urllib.request.Request(url, body, headers)\n\ntry:\n    response = urllib.request.urlopen(req)\n\n    result = response.read()\n    print(result)\nexcept urllib.error.HTTPError as error:\n    print(\"The request failed with status code: \" + str(error.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(error.info())\n    print(error.read().decode(\"utf8\", 'ignore'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4c678fd-b3f0-4686-ae1c-ac56d40c1c50","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"deploy-model-to-aml","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":326522936690748}},"nbformat":4,"nbformat_minor":0}
